---
title: "Chapter02_03.Rmd"
output: html_document
---
# Obtendo modelos de predição

```{r}
library(DMwR)
data(algae)
```
O principal objetivo desse estudo é obter predições para os valores das frequências das 7 algas num conjunto de 140 amostras. Como as frequências são números, esse é um problema de regressão. 

Esse procedimento pode ser utilizado para prever os valores das variáveis dependentes a partir de observações futuras das variáveis independentes ou para obter um melhor entendimento das interações entre as variáveis em nosso problema.

Neste caso, usaremos **Regressão Linear Múltipla** e **Árvores de Regressão**. Estes dois modelos são boas escolhas pois fazem diferentes suposições acerca da forma da função de regressão.

Como a regressão linear não consegue lidar com valores desconhecidos e as árvores de regressão lidam bem com essas lacunas, a preparação dos dados será diferente para cada método.

## Regressão Linear Múltipla

Neste caso será necessário eliminar os valores desconhecidos. Será efetuado o procedimento final da sessão anterior (Chapter02_02):

```{r}
algae <- algae[-manyNAs(algae),]
clean.algae <- knnImputation(algae, k = 10)
```

o DataFrame "clean.algae" não possui valores desconhecidos. Foram inseridos utilizando o método k vizinhos mais próximos discutidos no final da sessão anterior.

```{r}
clean.algae
```

Vamos obter aqui o modelo de regressão linear para a variável "a1", considerando as primeiras 12 variáveis como independentes.

```{r}
lm.a1 <- lm(a1 ~ ., data = clean.algae[, 1:12])
```

```{r}
summary(lm.a1)
```

Para trabalhar em modelos lineares com as variáveis categóricas nominais, o R cria novas variáveis "dummy", binárias (0 ou 1).

Por exemplo: "season" é uma variável que pode assumir 4 diferentes valores, a saber, "spring", "summer", "autumn" ou "winter". O R cria as seguintes variáveis: "seasonspring", "seasonsummer", "seasonautumn" e "seasonwinter" e atribui 0 ou 1 para indicar "ausência" ou "presença" respectivamente.

Coisas importantes a serem avaliadas no sumário acima: 

- R^2 ajustado: quanto mais próximo de 1 melhor; 
- F-statistic que testa a hipótese nula de não haver dependência, ou seja, para termos 95% de certeza de que não há dependência, o valor-p deve ser maior que 0,05... neste caso, é muito menor 2.444e-12, assim, pode-se concluir que Há dependência entre as variáveis.

Neste caso temos um impasse: o coeficiente de explicação R^2 ajustado é muito baixo, aprox. 32%; mas a estatística F não nos permite descartar a dependência. Precisamos nos aprofundar na análise e elimar as variáveis não relacionadas (ruídos)... a técnica é conhecida como "Backward Elimination"

## Backward elimination

1º passo: aplicar uma análise de variância:

```{r}
anova(lm.a1)
```

Essa análise mostra que a variável "season" é a que menos contribui para a redução do ajuste do modelo. Será eliminada.


```{r}
lm2.a1 <- update(lm.a1, . ~ . - season)
summary(lm2.a1)
```

O ajuste aumentou um pouco, para 32,7%, mas ainda é pequeno... 

vamos comparar os dois modelos utilizando uma análise de variância:

```{r}
anova(lm.a1, lm2.a1)
```

A soma do erro quadrático dimunuiu (-448), mas a comparação  mostra que a diferença não é significativa, um valor de 0,6971 nos diz que apenas temos algo em torno de 30% de confiança (1 - 0,6971) de que há uma diferença.

Para verificar que outras variáveis poderíamos excluir, poderíamos fazer uma ANOVA de lm2.a1 e assim por diante, mas o R possui uma função que faz todo esse processo de forma automática.

Vejamos:

```{r}
final.lm <- step(lm.a1)
```
```{r}
summary(final.lm)
```

A explicação desse modelo ainda assim não é interessante, aproximados 33%. Neste caso, temos um sinal de que a presumida linearidade do modelo não é adequada e deve ser descartada.

Entretanto cabe a explicação da equação. Seria assim:

a1 = 57.28555 + 2.80050 * sizemedium + 10.40636 * sizesmall - 3.97076 * mxPH - 0.05227 * Cl - 0.89529 * NO3 - 0.05911 * PO4

## Árvores de regressão

Como o método "árvores de regressão" lida bem com valores desconhecidos, só precisamos remover as amostras 62 e 199 por serem muito incompletas.

Vamos lá:

```{r}
library(rpart)
data(algae)
algae <- algae[-manyNAs(algae),]
rt.a1 <- rpart(a1 ~ ., data = algae[ , 1:12])
```

O pacote "rpart" implementa "árvores de regressão" no R. A sintaxe para obter o modelo é simular ao "lm".

Vamos ver os resultados:

```{r}
rt.a1
```

Vamos entender isso:

A "árvore de regressão" fornece uma hieraquia de testes lógicos sobre algumas das variáveis independentes (o método seleciona automaticamente as mais relevantes). 

É interessantíssimo o modelo... num momento pode ser interessante traduzir isso:

"A regression tree is a hierarchy of logical tests on some of the explanatory
variables. Tree-based models automatically select the more relevant variables;
thus, not all variables need to appear in the tree. A tree is read from the root
node that is marked by R with the number 1. R provides some information
of the data in this node. Namely, we can observe that we have 198 samples
(the overall training data used to obtain the tree) at this node, that these
198 samples have an average value for the frequency of algal a1 of 16.99, and
that the deviance30 from this average is 90401.29. Each node of a tree has two
branches. These are related to the outcome of a test on one of the predictor
variables. For instance, from the root node we have a branch (tagged by R
with \2)") for the cases where the test \PO4≥43.818" is true (147 samples);
and also a branch for the 51 remaining cases not satisfying this test (marked
by R with \3)"). From node 2 we have two other branches leading to nodes 4
and 5, depending on the outcome of a test on Cl. This testing goes on until
a leaf node is reached. These nodes are marked with asterisks by R. At these
leaves we have the predictions of the tree. This means that if we want to use
a tree to obtain a prediction for a particular water sample, we only need to
follow a branch from the root node until a leaf, according to the outcome of
the tests for this sample. The average target variable value found at the leaf
we have reached is the prediction of the tree."


É possível ver isso de forma gráfica... No pacote "DMwR" existe a função prettyTree() para essa finalidade

```{r}
prettyTree(rt.a1)
```

A função summary() também pode ser aplicada. Vejamos:

```{r}
summary(rt.a1)
```

Ver como evitar o "sobreajuste". p. 74 (88) a partir de "The function rpart() that we have used to obtain our tree only grows the
tree, stopping when certain criteria are met. Namely, the tree stops growing
whenever (1) the decrease in the d"